# JobPulse

> **Local-first LLM job intelligence pipeline with fallback orchestration, structured validation, and personalized skill-gap reporting.**

JobPulse is an end-to-end LLM system that transforms raw job postings into structured intelligence and actionable reports. It combines scraping, structured extraction, validation, model fine-tuning, orchestration, and reliability engineering into a single reproducible workflow.

------

# ğŸš€ What JobPulse Does

JobPulse:

- Scrapes job postings (Handshake + extensible connectors)
- Extracts structured requirements using LLMs
- Fine-tunes compact models using LoRA
- Validates structured outputs with QC gates
- Falls back across providers (Local â†’ API)
- Generates personalized skill-gap reports
- Exposes functionality via MCP tools
- Orchestrates workflows with LangGraph

------

# ğŸ— Architecture Overview

```
Scrape â†’ Clean â†’ Extract â†’ Validate â†’ Fallback (if needed) â†’ Report â†’ Persist Artifacts
```

### Design Principles

- **Local-first inference**
- **Provider abstraction (OpenAI-compatible APIs)**
- **Strict schema validation**
- **Structured artifact logging**
- **Traceable execution**
- **Composable orchestration**

------

# âœ¨ Key Capabilities

## ğŸ” Job Data Pipeline

- Playwright-based scraping
- Multi-platform connector abstraction
- SQLite storage
- Raw â†’ Structured â†’ Report artifact flow

## ğŸ¤– LLM Extraction

- Local HuggingFace inference (PyTorch)
- LoRA fine-tuned Qwen 0.5B
- OpenAI-compatible API backends
- Prompt versioning
- Robust JSON repair + schema validation

## ğŸ” Reliability Engineering

- Local-first extraction strategy
- Automatic cloud fallback
- QC validation gate before report generation
- JSON repair heuristics:
  - Code fence stripping
  - Tail extraction
  - Bracket repair
  - Balanced truncation
- Step-level trace logging

## ğŸ§  Orchestration Layer

- LangGraph state machine
- Conditional routing
- MCP tool server (stdio transport)
- Short-lived MCP sessions for stability

------

# ğŸ“‚ Project Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ artifacts/          # Structured outputs + trace logs
â”œâ”€â”€ scripts/                # CLI entrypoints & experiments
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ connectors/         # Platform scraping adapters
â”‚   â”œâ”€â”€ extractors/         # Local extraction logic
â”‚   â”œâ”€â”€ llm/                # Prompt + provider abstraction
â”‚   â”œâ”€â”€ mcp_server/         # Tool-based MCP server
â”‚   â”œâ”€â”€ orch/               # LangGraph orchestration
â”‚   â”œâ”€â”€ schemas/            # Pydantic structured models
â”‚   â”œâ”€â”€ training/           # LoRA fine-tuning pipeline
â”‚   â”œâ”€â”€ report.py           # Markdown report generation
â”‚   â””â”€â”€ db.py               # SQLite persistence
â””â”€â”€ models/                 # LoRA adapters (generated)
```

Artifacts generated per job:

```
data/artifacts/<job_id>/
  structured.json
  qc.json
  report.md
  trace.json
```

`trace.json` records:

- Extraction path (local or API)
- QC result
- Fallback events
- Report generation metadata

------

# âš™ï¸ Environment Setup

## 1ï¸âƒ£ Install Dependencies

Project uses `uv` for dependency management.

```
uv sync
```

## 2ï¸âƒ£ Install Playwright Browsers (Required)

```
python -m playwright install --with-deps
```

âš ï¸ Without this step, scraping will fail.

## 3ï¸âƒ£ Set API Keys (Optional if using local-only)

```
export OPENAI_API_KEY="your_key"
export NVIDIA_API_KEY="your_key"
```

------

# ğŸš€ Usage

## 1ï¸âƒ£ Scrape Job Postings

```
uv run python scripts/run_pipeline.py --pages 1 --limit 10
```

Stored in:

```
data/db/jobs.db
```

------

## 2ï¸âƒ£ Run Single Job via MCP Tool Chain

```
uv run python scripts/run_one_job_mcp.py \
  --job-id 10704289 \
  --provider openai
```

------

## 3ï¸âƒ£ Run LangGraph Orchestration (Local-first + Fallback)

```
uv run python scripts/run_graph_one.py \
  --job-id 10704289 \
  --local-first \
  --local-model Qwen/Qwen2.5-3B-Instruct \
  --local-mode plain \
  --extract-provider openai \
  --report-provider openai
```

------

# ğŸ§  LoRA Fine-Tuning

## Dataset

Located in:

```
src/training/datasets/
  jd_struct_train.jsonl
  jd_struct_val.jsonl
```

Generated from teacher model outputs.

## Train

```
uv run python src/training/train_lora.py
```

## Output

LoRA adapter saved to:

```
models/qwen2.5-0.5b-jd-lora/
```

------

# ğŸ”„ Provider Support

JobPulse supports OpenAI-compatible backends:

| Provider | Example Model        |
| -------- | -------------------- |
| OpenAI   | gpt-4o-mini          |
| NVIDIA   | moonshotai/kimi-k2.5 |
| Local HF | Qwen + LoRA          |

Example NVIDIA endpoint:

```
https://integrate.api.nvidia.com/v1
```

------

# ğŸ›  MCP Tool Server

Start manually:

```
python -m src.mcp_server.server
```

Available tools:

- `fetch_jd`
- `extract_local`
- `extract_api`
- `qc_validate`
- `generate_report_api`

------

# ğŸ“ˆ Reliability Strategy

## Extraction Flow

```
local â†’ qc_fail â†’ api â†’ qc_pass â†’ report
```

## JSON Hardening

- Code fence stripping
- Tail extraction
- Bracket repair
- Balanced truncation
- Final brace-matching fallback

This significantly reduces malformed LLM output failure rates.

------

# ğŸ§© Tech Stack

- Python
- PyTorch
- HuggingFace Transformers
- PEFT (LoRA / QLoRA)
- LangGraph
- MCP
- Playwright
- SQLite

------

# ğŸ¯ Engineering Highlights

This project demonstrates:

- Production-style LLM system design
- Model fine-tuning workflow
- Provider abstraction layer
- Structured validation patterns
- Fallback orchestration
- Artifact-based observability
- Reliability engineering for LLM output

------

# ğŸ”® Future Improvements

- Batch graph runner
- Structured extraction evaluation dashboard
- Resume ingestion + automated skill-gap comparison
- Async scraping
- Dockerized deployment
- Monitoring & metrics layer